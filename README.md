# Capgemini Real-Time ML Platform (Portfolio Project)

**Author:** Saketh Sai Nigam Kanduri â€” Software Engineer, Capgemini

A production-style reference project showing how to serve ML predictions via a FastAPI microservice with CI/CD, containerization, Helm manifests, and basic observability.

## ðŸš€ Features
- FastAPI scoring API (`/score`) with Pydantic validation
- Simple feature engineering and a lightweight logistic model (pre-trained artifact included)
- Prometheus metrics (`/metrics`) and health checks (`/health`)
- Dockerfile + docker-compose for local run
- Helm chart for Kubernetes/OpenShift deployments
- CI with GitHub Actions + example Jenkinsfile
- Pytest tests

## ðŸ§­ Structure
```
capgemini-rt-ml-platform/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ features.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ artifacts/model.pkl
â”‚   â””â”€â”€ train_dummy_model.py
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ helm/
â”‚       â””â”€â”€ capgemini-rt-ml-platform/
â”‚           â”œâ”€â”€ Chart.yaml
â”‚           â”œâ”€â”€ values.yaml
â”‚           â””â”€â”€ templates/
â”‚               â”œâ”€â”€ deployment.yaml
â”‚               â”œâ”€â”€ service.yaml
â”‚               â””â”€â”€ hpa.yaml
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ tests/test_api.py
â”œâ”€â”€ scripts/load_sample_requests.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ðŸ§ª Quickstart

### 1) Python (local)
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8080
```
Then open: `http://localhost:8080/docs`

### 2) Docker
```bash
docker build -t capgemini-rt-ml-platform:latest .
docker run -p 8080:8080 capgemini-rt-ml-platform:latest
```

### 3) Kubernetes with Helm
```bash
helm upgrade --install rt-ml infra/helm/capgemini-rt-ml-platform -n default
```

## ðŸ§© Sample Request
```bash
curl -X POST http://localhost:8080/score -H "Content-Type: application/json" -d '{
  "features": [0.3, -1.2, 0.5, 1.1, 0.0, -0.4]
}'
```

## ðŸ“Š Observability
- **/metrics**: Prometheus metrics (request count, latency histogram)
- **/health**: liveness check

## ðŸ§ª Tests
```bash
pytest -q
```

## ðŸ“„ Notes
- The included model is a lightweight logistic function with random weights (AUC placeholder ~0.85) to keep the repo self-contained.
- To integrate a real MLflow model, replace `models/train_dummy_model.py` and `app/model.py` with MLflow loading logic.
